\documentclass[12pt]{article}
\usepackage{PDReport}
\usepackage{mathmacros}
\usepackage{siunitx}
\usepackage{xcolor, colortbl}
\usepackage{lipsum} % remove this

% https://scivision.co/matplotlib-in-cygwin-64-bit/

% My information
\const{faculty}{Faculty of Physics \& Astronomy}
\const{reportTitle}{Acoustic Modelling Using Mel-Frequency Cepstral Coefficients}
\const{fullName}{Thomas C. Fraser}
\const{term}{3A}
\const{program}{Mathematical Physics}
\const{studentID}{20460785}
\const{address}{154 Quarry Ave.}
\const{city}{Renfrew, Ontario}
\const{postalCode}{K7V 2W4}
\const{employer}{Sysomos}
\const{employerCity}{Toronto, Ontario}

% Title Page (ignore)
\newcommand{\createtitlepage} {
  \pagenumbering{roman}
  \waterlootitlepage
    {\faculty}
    {\reportTitle}
    {\employer \\ \employerCity}
    {\fullName \\ \term \ \program \\ ID \studentID \\ \today}
}

% Letter of Submittal
\newcommand{\letterofsubmittal} {
  \waterlooletterofsubmittal
    {\address}{\city}{\postalCode}{\fullName}{\studentID}{
    I have prepared the enclosed report ``\reportTitle'' as my \term Work Term Report for my work term spent at \employer in \employerCity. This is my fourth work term report.

    The purpose of this report is to summarize the research done by me in order to determine how to classify audio signals in two categories: music and speech. I aim to convince anyone planning to perform human speech-related modelling to consider implementing algorithms to calculate Mel-Frequency Cepstral Coefficients (MFCCs). MFCCs are a form of spectral analysis on spectra themselves and thus the reader should have some experience with mathematical techinques like Fourier Transforms. This report explains the motivations behind MFCCs in detail.

    \employer is currently a leading provider of social media data analytics for large business clients like Coca-Cola, Adidas, etc.

    This report was written entirely by me and has not received any previous academic credit at this or any other institution. I give permission to \employer to keep a copy of this report on file and use it as necessary in the future.
    }
    {
     \scalebox{0.11}{\includegraphics{sign.png}}
    }
}

% Summary
\newcommand{\summary}{
  \tocsection{Summary}
  {
  This report, titled ``\reportTitle'', aims to summarize the research conducted by me as a member of the Research Labs team at Sysomos during the Fall 2015 work term in the context to the Audio Analytics project. It focuses on acoustic modelling of social media video in order to perform a high-level categorization of social video into groups like music, speech, laughing, cheering, silence, etc. It demonstrates the effectiveness of using Mel-Frequency Cepstral Coefficients (MFCCs) to build acoustic models to predict the class of each video.

  To begin, this report provides the background into the domain of social media data and Sysomos's motivations for this project. It summarizes the first attempts to use external APIs and libraries to perform speech recognition on the audio clips. By doing this, it motivates and outlines the proposed pipeline for audio data augmentation. Secondly, MFCCs are described in detail in order to provide insight into why MFCCs are often used for acoustic modelling applications. Finally, MFCCs are quantitatively compared to a few dozen other audio signal features by performing a binary classification task on two seperate data sets; a collection of popular social media video and an academic data set of audio clips taken from the 2015 MIREX competition. These features are then ranked by classification accuracy using a single feature SVM model with $10$-fold cross-validation.

  The report concludes by interpreting the results of these experiments in detail. It reveals that MFCCs effectively model the way humans produce and comprehend speech. Moreover, MFCCs are shown to perform well in classification tasks other than Automatic Speech Recognition (ASR). The quantitative analysis reveals that MFCCs consistently out perform other statistical and musical features considered and attempts to use beat extraction to perform this type of classification is not very successful.

  Finally, a subset of potential areas for future work to continue this research are listed.
  }
  \newpage
}

% Introduction
\newcommand{\introduction}{
  \fancyhf{}
  \pagestyle{fancy}
  \renewcommand{\headrulewidth}{0pt}
  \rfoot{\thepage}
  \pagenumbering{arabic}
  \mySection{Introduction}
  {
    Sysomos is a Toronto based company with secondary offices all across the world. They are a leader in social media management and have over 1000 high-profile clients. Their primary business is gathering and collecting data and actionable insights from social media platforms like Twitter \cite{twitter}, Facebook \cite{facebook}, Tumblr \cite{tumblr}, Vine \cite{vine}, and Instagram \cite{instagram}. Their computational resources allow for the ingestion and analysis hundreds of billions of data sources in real-time \cite{sysomos}. Sysomos products give clients an ability to understand and visualize their target demographic/audience for various marketing and public-relations projects.

    The general ambition of the Research Labs team at Sysomos is to examine the corpus of all social media data to try and discover new ways to uncover information beneficial to the core products of the company. Typical features of social posts used to build meaningful stories include tweets, photos, comments, friendships and conversations; anything that can be found online.

    One of the projects that I was fortunate enough to lay foundations for was our Audio Analytics project. Quite ambitiously, the project involved tackling the question: \textit{How can we utilize the audio channel of social media videos to augment or enhance the existing data?} Augmenting data is universally useful for Sysomos products as it allows for better indexing and searchability of content. Additionally, extra data bares revenue streams that Sysomos and similar companies in the industry have yet to tap into.

    The long-term goals of the project are as follows:
    \begin{enumerate}
      \item Perform automatic speech recognition (ASR) on the audio in order to extract phrases spoken by individuals.
      \item Determine what music genres individuals are interested in for marketing purposes.
      \item Predict which video frames are most interesting/characteristic of the entire video so that image analytics can be performed efficiently.
    \end{enumerate}

    None of these problems are fully solved by the research conducted in this report. However significant strides are made by tackling an easier problem: \textit{How can we predict whether or not a given video contains music, speech, laughing, cheering, silence, etc?} This categorization indirectly contributes to each of the above three goals. This report focuses on acoustic modelling through the use of audio features called Mel-Frequency Cepstral Coefficients (MFCCs). It will assume the reader has a basic knowledge of machine learning concepts such as generic classification and regression models, support vector machines, feature extraction, accuracies, precision, recall, and F1-scores. Additionally, a small amount of spectral theory will be required.
  }
  \newpage
}
\newcommand{\analysis}{
  \mySection{Background}{
    Currently, Sysomos is actively analyzing text and relationships between users. Only recently has Sysomos moved into the industry of image processing through the acquisition of Gaze Metrics \cite{sysomosgaze}. Naturely, audio analytics on video is the next step for Sysomos. Sysomos's Audio Analytics project, led by the Research Labs Team and I aims to expose as much information from the audio signal of the video as possible. Consequently, this project is discarding the visual channel of the video entirely. Research conducted at Standford University shows promise that deep-learning models that combine both audio and visual modalities of a video perform better at classification tasks \cite{multimodal}; however their techniques are currently beyond the scope of the project.
    \mySubSection{Social Data is Not Academic Data}{
      Before going further into the analysis, it is important to recognize the context of social media video. Automatic speech recognition tasks are conducted in scenarios where humans are speaking to a machine. For example: when using an iPhone, you speak to Siri \cite{siri} or on windows 10 you speak to Cortana \cite{cortana}. In either case, the end-user understands that the speech recognition algorithms perform better when speaking clearly and with little to no noise. Social media audio is not as nice to work with. Often times, many complications arise that diminish the integrity of the audio. For instance there can be loud music playing in the background, screaming, numerous speakers, different languages, or speech that is inaudible. Speech recognition performs well in an academic setting where speech data sets are normalized and idealized versions of what might be found on social media. As a result, the task of Automatic Speech Recognition is currently also outside the scope of the project, but is intended to be added in the future.
    }
    \mySubSection{Automatic Speech Recognition}{
    \label{sec:asr}
      Although this report will not discuss any results associated with Automatic Speech Recognition (ASR), it is important to understand how acoustic modelling fits into the scope of ASR. Acoustic modelling will prove useful for many other audio classification tasks but is also a core part of and ASR pipeline. There are three main components to a ASR pipeline \cite{asr}. Firstly, the acoustic model takes in the raw audio signal and, loosely speaking, performs dimensionality reduction in order to produce a vector of values that characterizes the phonemes (parts of speech) being spoken. Secondly, a pronounciation model determines how likely phonemes are to occur ajacent to one another while someone is speaking. It should then predict what words correspond to a group of phonemes. Pronounciation models are language dependent and are typically Hidden Markov Models (HMMs). Finally, a language model measures how probable two words are to be spoken sequentially. It will then construct phrases using words generated by the pronounciation model.\\
    }
    \mySubSection{Solution Exploration}{
      \label{sec:solutionexploration}
      Speech recognition technologies are plentiful. One economic way of improving Sysomos's audio data stream would be to simply utilize another service that does speech recognition automatically. Early explorations into this idea were not successful. First, publicly available speech apis, namely Google \cite{apigoogle}, wit.ai \cite{apiwit}, IBM Watson \cite{apiibm}, and AT\&T \cite{apiatt}, were too limited for the scope of the social audio project. Also, lincensed software library built for pronounciation and language modelling like Julius \cite{pacjulius}, CMU Sphinx \cite{paccmu}, Kaldi \cite{packaldi} and Nuance Dragon \cite{pacdragon} were all found to be incompatible or too expensive for social media audio. Third and final, captioning services like VoiceBase \cite{capvoicebase}, Amara \cite{capamara}, CaptionSync \cite{capcaption} and 3PlayMedia \cite{cap3play} actually provide transcripts of audio recordings by hand; which isn't feasible for millions of videos. As a result of a failed exploration into existing solutions, it was decided that a smaller problem, although equally useful, should be tackled first.
    }
  }

  \mySection{Proposed Pipeline}{
    Upon consideration of the limitations discovered in section \ref{sec:solutionexploration}, a high-level classification on the audio is required. This categorization is sub-divided into 4 stages. First, download the videos from its respective social media website. Next, extract the audio from the video file using the popular, cross-platform, media manipulation tool called FFmpeg \cite{ffmpeg}. Afterward, the raw audio signal needs to be normalized. Finally, the normalized audio signal is passed into a high-level classification model.
    \mySubSection{Normalizing Signal}{
      In the context of sound as a continous pressure wave \cite{sound}, the \textit{intensity} of a sound wave is a continous pressure signature $x(t)$ (where $t$ is time and $x$ is the relative measure of the displacement of a speaker or microphone diaphram). In order to digitalize the signal, it is typically sampled around $\SI{44.1}{\kilo\hertz}$. This is the Nyquist frequency \cite{nyquist} corresponding to twice the maximum human hearing frequency of around $\SI{20}{\kilo\hertz}$ \cite{humanhearing}. Typically, an audio signal is broken into two channels; one intended for the right ear and one for the left $\vec{x}[t] = \br{x_L[t], x_R[t]}$. Furthermore, the values encoded in $x(t)$ are always $b$-bit integers. In order to normalize for any bitrates, one must take the average of the two channels, and then divide by the maximum intensity. Namely,
      \[ x[t] \defined \f{x_L[t] + x_R[t]}{2\cdot2^{b-1}} \numberthis \label{eq:normalizedsignal}\]
      Henceforth, \eqref{eq:normalizedsignal} will be considered the audio signal to be analyzed. The only remaining feature of $x\bs{t}$ that isn't normalized at this stage is the sampling frequency $f$.
    }
    \mySubSection{High-Level Classification}{
      \label{sec:highlevelclassification}
      After normalizing the audio, it is the goal of this project to classify the clip into one or more categories. As an example, does the clip contain music, speech, laughing, cheering, silence, etc? Answering this question will require acoustic modelling techniques, such as MFCCs.
    }
  }

  \mySection{Mel-Frequency Cepstral Coefficients Disected}{
    \label{sec:MFCCs}
    Mel-Frequency Cepstral Coefficients (MFCCs), developed in 1974 by Bridle, Brown and Mermelstein \cite{MFCCfoundation1,MFCCfoundation2,MFCCfoundation3}, are a vector of real-valued features that correspond to a short window of time within an audio signal. They are a respresentation of the unique \textit{phoneme} being spoken during that short window. Phonemes, a term used in the study of linuistics, are the irreducible sound elements made when speaking. English has 44 phonemes such as /m/ as in \textit{man, summer, palm} or /ow/ in \textit{now, shout, bough}. What follows is a detailed exposition on how MFCCs are calculated. This is done to accomplish two things. Firstly, to outline how to implement MFCCs and identify areas that can be explored for further improvement. Secondly, this section aims to convince the reader that MFCCs are well-motivated in their construction for use in acoustic modelling. A quantitative analysis of their importance will follow in section \ref{sec:performanceevaluation}.

    \myFigure{0.7}{findings/speechmusic.png}{Comparison between speech (woman speaking) and music (classical) waveforms and spectrograms. Taken from the MARSYAS ``Music Speech'' database. \cite{marsyas} \label{fig:speechmusic}}

    \myFigure{0.7}{findings/speechzoomed.png}{Zoomed in portion of figure \ref{fig:speechmusic}. MFCCs characterize repeating red bands in this figure. \label{fig:specband}}
    \mySubSection{Windowing}{
      Since phonemes spoken are considered \textit{stationary waves} on the time length less than around $\sim\SI{30}{\milli\second}$ \cite{ms}, the total audio signal $x[t]$ given by \eqref{eq:normalizedsignal} needs to be divided up into shorter intervals of time. Notice in figure \ref{fig:specband}, the spectrogram is approximately constant within the outlined interval. MFCCs characterize the structure of repeating high-intensity (red) bands. This structure is unique for each phoneme. An appropriate window size ($N$) should be chosen based off the sampling frequency of the raw signal. Choosing a window length of around $1024$ samples is useful for two reasons:
      \begin{enumerate}
        \item Powers of $2$ require no padding when taking a Discrete Fourier Transform \cite{fastfourier}.
        \item At a sampling rate of $\SI{44.1}{\kilo\hertz}$, $1024$ samples corresponds to $\sim\SI{20}{\ms}$ which is completely sufficient the time scale considered.
      \end{enumerate}
      Before performing a fast fourier transform on a finite interval of time, an \textit{apodization window function} needs to be applied to minimize leakage artifacts induced by the periodic extension of the signal \cite{dftox}. For the purposes of MFCCs, the popular Hamming Window \cite{hamming} works just fine.

      MFCCs are computed for each window of samples. Let $m_{w}^{\br{0}}$ represent the vector of values of contained within the window $w$. The `$(0)$' indicates the zeroth stage of the MFCC computation. Thus $m_{w,j}^{\br{0}}$ are the individual values of intensity $0 \leq j \leq N - 1$
    }
    \mySubSection{Discrete Fourier Transform}{
      Next, in order to expose the frequency domain of the short window $w$, perform a real-valued Discrete Fourier Transform \cite{discretefourier}.
      \[ m_{w,k}^{\br{1}} = \f1N \abs{\sum_{j=0}^{N-1} m_{w,j}^{\br{0}} e^{-2\pi i kj/N}} \quad 0 \leq k \leq N-1 \]
      Now $m_{w,k}^{\br{1}}$ measures the contribution made to the signal $m_{w}^{\br{0}}$ at frequency $\f{k}{N} \cdot f$ (where $f$ is the sampling frequency of the original signal). Note $m^{\br{0}}$ is in the time-domain while $m^{\br{1}}$ is in the frequency domain. Note however that in principle, a Fast Fourier Transform is taken to reduce the time complexity of the computation from $O(N^2)$ to $O(N\log N)$ \cite{fastfourier}.
    }
    \myFigure{0.7}{findings/melscale.png}{Mel Scale vs. Hertz Scale \label{fig:melscale}}
    \myFigure{0.7}{findings/windowing.png}{Triangular Windowing on Frequency Domain\label{fig:triangularwindowing}}
    \mySubSection{Mel-Scale \& Triangular Windowing}{
      Now that the frequency domain is revealed via $m_{w}^{\br{1}}$, MFCCs shift the standard frequency scale from Hertz to Mels. The Mel frequency scale, introduced in 1937 \cite{melfoundations}, is a logarithmic scale associated with the way humans perceive pitch. At larger frequencies, increasingly large frequency intervals are perceived by humans to be equal pitch increments \cite{melfoundations}. The scale is defined with respect to $1000 \text{mels} = 1000 \text{Hz}$. Figure \ref{fig:melscale} shows this relationship.
      \[ m = \f{1000}{\log{2}}\log{\br{1+\f{f}{1000}}} \]
      In order to normalize the signal $m_{w}^{\br{1}}$ further, it is passed through a set of triangular windows. This is illustrated in figure \ref{fig:triangularwindowing}. Letting each triangular window be denoted $T_j$, there are typically a few dozen windows ($N_T$) (leading research indicates 40 is optimal \cite{mirex2015winners}). Thus $j$ is an integer with $0 \leq j \leq N_T - 1 \in Z$. This step in the computation of MFCCs can be expressed in equation \eqref{eq:triangularwindowing}.
      \[ m_{w,j}^{\br{2}} = \sum_{k=0}^{N-1} T_{j, k} m_{w,k}^{\br{1}} \numberthis \label{eq:triangularwindowing}\]
      In equation \eqref{eq:triangularwindowing}, $T_{j, k}$ represents the value of triangular window $j$ at the frequency $\f{k}{N} \cdot f$. Thus $m_{w,j}^{\br{2}}$ is the response from window $T_j$ and is still in the frequency domain. Notice in figure \ref{fig:triangularwindowing} the centers of the triangles are spaced in equally on the mel-scale, but logarithmically spaced on the hertz-scale. Furthermore, to normalize the response from each window $T_j$, the height of the triangular is chosen such that all windows have equal area. The width of the window is determined by the neighboring centers. Also note that the windows have freqeuncy range $\sim 100 - 2500 \text{Hz}$. This range has a lot of freedom but is approximately the range of human voice production \cite{humanvoicerange}.
      Finally, the logarithm of each $m_{w}^{\br{2}}$ is taken to normalize the difference between the results from each window \cite{computingmfcc}.
      \[ m_{w}^{\br{3}} = \log\br{m_{w}^{\br{2}}} \]
    }
    \mySubSection{Discrete Cosine Transform}{
      \label{sec:dct}
      Now that we have a vector ($m_{w}^{\br{3}}$) that characterizes the periodic behaviour of the signal in \textit{time} we can explore the periodic nature of the signal in the \textit{frequency} domain. This is the key component that differentiates MFCCs from typical signal analysis features. In order to accomplish this, we can perform yet another discrete fourier transform. However, in principle, only a discrete cosine transform is sufficient because the current signal is real-valued $m_{w}^{\br{3}} \in \R$ and the output is required to be real.
      \[ m_{w,j}^{\br{4}} = \sum_{j=0}^{N_T-1}m_{w, j}^{\br{3}} \cos\bs{\f{k\br{2j+1}\pi}{2N_T}} \quad 0 \leq k \leq N\tsb{mfcc}-1 < N_T \numberthis \label{eq:dct} \]
      Performing a discrete cosine transform in equation \eqref{eq:dct} moves $m_{w}^{\br{3}}$ from the frequency domain to $m_{w}^{\br{4}}$ in the \textit{quefrency} domain, which has units of time but is not correlated with the initial time domain. Just as the discrete fourier transform exposed the \textit{spectral} domain of the signal, \eqref{eq:dct} exposes the \textit{cepstral} domain of the signal. It is very important to note that $k$ takes on only $N\tsb{mfcc}$ values. Thus $m_{w}^{\br{4}}$ is a vector of length $N\tsb{mfcc}$. MFCCs act as a low-pass filter on the quefrency domain as only the smallest $N\tsb{mfcc}$ quefrency values are kept. This smooths out the representation of the vector $m_{w}^{\br{3}}$ because it removes high-quefrency noise artifacts. Typically, it is customary to select the first $N\tsb{mfcc} = 13$ cofficients \cite{speakergender,mirex2015winners}.
    }
    \mySubSection{Deltas \& Delta-Deltas}{
      The values obtained in \ref{sec:dct}, namely $m_{w}^{\br{4}}$, are called the \textit{Mel-Frequency Cepstral Coefficients}. They are a vector of $N\tsb{mfcc}$ real values for each window $w$. For the purposes of the analysis in section \ref{sec:performanceevaluation}, these are considered as the final MFCCs.
      \[ \text{MFCC}_{w} = m_{w}^{\br{4}} \]
      Nevertheless, research suggests that the human brain determines what phonemes are spoken by context of the sounds produced nearby in time \cite{temporallearning, temporaldiflearning}. Effectively, the trajectory of the MFCC vector contributes to the cognitive understanding of the spoken sounds. Often it is common to introduce the notion of \textit{deltas} and \textit{delta-deltas}; the discrete velocity and acceleration of the MFCCs respectively.
      \[ \text{MFCC}_{w} = \bs{m_{w}^{\br{4}}, \Delta m_{w}^{\br{4}}, \Delta^2 m_{w}^{\br{4}}} \]
      Where $\Delta m_w = m_{w+1} - m_{w-1}$ and $\Delta^2 m_w = \Delta m_{w+1} - \Delta m_{w-1}$ while appropriately handling boundary cases.
    }
  }

  \mySection{Performance Evaluation}{
    \label{sec:performanceevaluation}
    Section \ref{sec:MFCCs}outlined the motivation for MFCCs from the audial, cognitive-psychological perspective as well as how to compute them using spectral and cepstral analysis. This section aims to discuss some of the work done by this project to determine how well MFCCs perform in classification and regression problems compared to other typical features (see section \ref{sec:featurelist}). It will describe the methods used to compare these features and interpret the results of those tests.

    \mySubSection{Feature List}{
      \label{sec:featurelist}
      MFCCs have been shown to perform well in speech recognition tasks. The purpose of this performance evaluation is to compare the performance of MFCCs against a number of other statistical and musical features of audio signals. Table \ref{tab:features} lists and describes these features. This list is by no means exhaustive, but it aims to give a wide range comparisons. There are $4$ time-based features, $9$ frequency-based features, $12$ musical-based features, and the $13$ MFCCs for a total of $38$ features.

      \definecolor{Gray}{gray}{0.85}
      \definecolor{LightCyan}{rgb}{0.88,1,1}
      \begin{table}
      \label{tab:features}
      \caption{Table of features used in performance evaluation tests.}
      \vspace{0.05in}
      \begin{tabular}{|l|l|l|}
        \hline
        \rowcolor{Gray}
        \textbf{Feature Name} & \textbf{Desciption} \\
        \hline
        \rowcolor{LightCyan}
        \multicolumn{2}{|c|}{Time-Based Features} \\
        \hline
        Zero-Crossing Rate & Number of times signal crosses zero  \\
        & $\text{zcr}\br{x\bs{t}} = \f{1}{N - 1}\sum_{t=1}^{N-1}\mathbb{I}\bc{x\bs{t}x\bs{t-1} < 0}$ \\
        \hline
        Energy & Energy of discrete time signal \\
        & $\text{energy}\br{x\bs{t}} = \f{1}{N}\sum_{t=1}^{N-1}x\bs{t}x^*\bs{t}$ \\
        \hline
        Root-Mean Squared & Quadratic mean of signal \\
        & $\text{rms}\br{x\bs{t}} = \sqrt{\f{1}{N}\sum_{t=1}^{N-1}x^2\bs{t}}$ \\
        \hline
        Energy-Entropy & Shannon Entropy of sub-divided windows ($n = 10$) \\
        & $\text{H}\br{x\bs{t}, n} = - \sum_{i}^{n}\bc{e_i, \ln{e_i}}$ \\
        & $e_i = x\bs{t} / \text{energy}\br{x_i\bs{t}}$ \\
        \hline
        \rowcolor{LightCyan}
        \multicolumn{2}{|c|}{Frequency-Based Features} \\
        \hline
        Spectral Centroid & Center of mass of spectrum (Hz) \\
        & $\text{centroid}\br{X[n]} = \sum_{n=0}^{N-1}X[n]f(n)/\sum_{n=0}^{N-1}f(n)$ \\
        \hline
        Flatness or & $\text{flatness}\br{X[n]} = \exp\br{\f1N\sum_{n=0}^{N-1}\ln X[n]}/\f1N\sum_{n=0}^{N-1}X[n]$ \\
        Wiener Entropy & \\
        \hline
        Spectral Entropy & Energy-entropy of spectrum (see above) \\
        & $\text{H}\br{X\bs{t}, n} = - \sum_{i}^{n}\bc{e_i, \ln{e_i}}$ \\
        \hline
        Spectral Mean & Average of the spectrum (Hz) \\
        & $\bar{X} = \sum_{n=1}^{N}X[n]$ \\
        \hline
        Spectral Variance & Statistical variance \\
        & $\mathrm{Var} \br{X[n]} = \sum_{n=1}^{N}\br{X[n]-\bar{X}}^2$ \\
        \hline
        Spectral Kurtosis & Fourth standardized moment \\
        & $\text{Kurt} \br{X[n]} = \bar{X}_4/\mathrm{Var} \br{X}^2$ \\
        \hline
        Spectral Rolloff & 85\%-percentile of spectral energy \\
        \hline
        Spectral Skewness & Measure of left/right skewness\\
        & $\text{Skew} \br{X[n]} = \bar{X}_3/\mathrm{Var} \br{X}^{\br{3/2}}$ \\
        \hline
        Spectral Spread & Variance about spectral centroid (above) \\
        \hline
        \rowcolor{LightCyan}
        \multicolumn{2}{|c|}{Musical Features} \\
        \hline
        Chroma Coefficients & Maximum normalized histogram of frequency bins \\
        & centered around each of the 12 semitones $C, C\#, \ldots, B$ \\
        \hline
        \rowcolor{LightCyan}
        \multicolumn{2}{|c|}{Quefrency-Based Features} \\
        \hline
        MFCCs & Mel-Frequency Cepstral Coefficients (see \ref{sec:MFCCs}) \\
        \hline
      \end{tabular}
      \end{table}
    }
    \mySubSection{Feature Aggregration}{
      \label{sec:featagg}
      Initially, the audio is subdivided into windows of size $\sim 10\text{ms}$ and each of the $38$ features outlined in section \ref{sec:featurelist} is computed on those windows. However, in order to perform high-level classification tasks like the seperation of ``music'' and ``speech'' audio clips, the features need to be representative of the \textit{entire} clip, not just the windows. With this as motivation, each of the features per window were aggregated into \textit{second order} features (see table \ref{tab:secondorderfeatures}). The first two sets of second order features are respectively the mean and variance of the feature values per window. The remaining second order features explore the phase space of the feature signal; beats-per-minute (BPM) and a BPM confidence (see section \ref{sec:bpm}). In total each of the $38$ features were aggregated into $4\times 38 + 2 = 154$ second-order features. Note the $+2$ features are a tertiary aggregation on both BPM features.
      \begin{table}
        \label{tab:secondorderfeatures}
        \begin{center}
        \caption{Second order features breakdown.}
        \vspace{0.05in}
        \definecolor{Gray}{gray}{0.85}
        \begin{tabular}{|c|l|}
          \hline
          \rowcolor{Gray}
          \textbf{Count} & \textbf{Second Order Features} \\
          \hline
          38 & Mean of each feature \\
          \hline
          38 & Variance of each feature \\
          \hline
          38 & Predicted BPM (Beats per minute) for feature \\
          \hline
          38 & BPM Confidence on each feature \\
          \hline
          1 & Aggregrated expected BPM \\
          \hline
          1 & Aggregrated expected BPM confidence \\
          \hline
        \end{tabular}
        \end{center}
      \end{table}
    }
    \mySubSection{Beat Extraction}{
      \label{sec:bpm}
      In order to extract some information about the long-term repetition of a value-series, there are a few things that can be done \cite{tempo}. One possibility is constructing a recurrence plot (see figure \ref{fig:rplot}) for the feature vector. A recurrence plot is essentially an image where the pixel at the $i,j$ coordinate is given by equation \ref{eq:recc}.
      \[ R\bs{i,j} = \text{sim}\br{x[i], x[j]} \numberthis \label{eq:recc}\]
      Where the similarity measure is typically distance. Recurrence plots are always symmetric and their visual structure, specifically the diagonal strokes, encode the repetition in the signal $x$. However, as pointed out by \cite{rplot}, contruction and analysis of recurrence plots are highly non-linear; at least $O(n^2)$. Therefore, recurrence plots are not computationally feasible on large scales. \\
      \myFigure{0.7}{findings/recurrence.png}{\label{fig:rplot}Recurrence plots of different audio signals. Taken from \cite{rplot}.}
      Another very common option are comb-filters \cite{comb}. However, like recurrence plots, comb filters are inherently computationally slow (typically $O(n^2)$). \\
      As such, a fast, $O(n)$ algorithm was developed as part of this project for BPM prediction. The algorithm is illustrated in figure \ref{fig:bpmcalc}. The core idea of this algorithm is that distances between adjacent peaks should be evenly spaced if there is a consistent tempo to the audio signal. For each feature:
      \begin{enumerate}
        \item Perform delta peak detection using Eli Billauer's \textit{peakdet} algorithm originally developed for matlab \cite{peakdetect}.
        \item Ignore all minimums, only look at maximums (red dots in figure).
        \item Construct a histogram based on the distance between adjacent peaks.
        \item The predicted BPM is the largest column in histogram.
        \item The BPM confidence is the ratio of the largest histogram to the total number of data points.
        \item The aggregate BPM and confidence is given by 4. and 5. on the combined histogram for all features (see figure \ref{fig:aggbpm}).
      \end{enumerate}
      \myFigure{0.25}{findings/bbmcalc.png}{\label{fig:bpmcalc}BPM prediction using fast $O(n)$ algorithm.}
      \myFigure{0.4}{findings/aggbpm.png}{\label{fig:aggbpm}Aggregate BPM histogram.}
      \mySubSubSection{Issues with BPM Measurements}{
        It is crucial to notice that beat extraction is very sensitive to whole ratios of the true BPM value. Intuitively, if every other peak was missed by the algorithm, the predicted BPM would be $\f{1}{2}$ of the actual value (i.e. 160BPM and 80BPM should both be considered ``correct'' because the audio is likely composed of multiple channels of repetition). Furthermore, the sampling rate of the audio signal is required to be much faster than the BPM in order for it to be detected $f >> \text{BPM}$. Moreover, in order to perform a aggregation of each of the histograms accurately, the edges of the bins need to be aligned. For the purpose of this report, the bin width was defaulted to 10BPM and aligned with 0BPM.
      }
    }
    \mySubSection{Binary Classification}{
      \label{sec:binclass}
      In order to compare the performance of MFCCs with other features of section \ref{sec:featurelist} a two tier classification task was used. As outlined in section \ref{sec:highlevelclassification}, one of the important aspects of this projects pipeline is a high-level classification of between different audio environments. \\
      The classification problem proposed and used for the analysis of this report is the seperation of audio clips into two categories: music and speech.
      \mySubSubSection{Feature Ranking via Single Feature Classification Accuracy}{
        The first tier of the classification task was construction a support vector machine (SVM) classification model using a radial basis function (RBF). This was done for each of the second order features dicussed in section \ref{sec:featagg} and ranked by their $10$-fold cross-validation, classification accuracy. This allowed for the forward-selection of best, most correlated features to the two classes: music and speech.
      }
      \mySubSubSection{Multi-Feature SVM based on Feature Rankings}{
        After performing these rankings, the top $k$ features were selected and a multi-dimensional SVM model was trained and evaluated using $10$-fold cross-validation. The values of $k$ were allowed to vary to examine the accuracy convergence and signatures of overfitting. Figure \ref{fig:svmvisual} has two examples of a 2d SVM. The $x$ and $y$ axes are two features chosen at random. The figures are intended to illustrate the seperability of the data. Red dots are music clips and blue dots are speech clips. The orange radial line is the RBF decision function generated by training the SVM. \\

        Note: $k$-nearest neighbors models (KNN) and logistic/linear regression models were also performed with very similar results. As such, they have been omitted from this report.
      }
      \myFigure{0.55}{findings/svmvisual.png}{Two Example SVMs with RBF performed on the MIREX data set. Red = Music; Blue = Speech \label{fig:svmvisual}}
    }
    \mySubSection{Data Sets}{
      The binary classification problem and methodology discussed in section \ref{sec:binclass} was applied to two independent data sets. One taken from social media, and one taken from the annual Music Information Retrieval Evaluation eXchange (MIREX) contest.
      \mySubSubSection{Social Data}{
      \label{sec:socialdataset}
        For the social media data set, approximately 2800 of the most popular videos from December 2015 were downloaded from Twitter, Tumblr, Vine and Instagram. These were classified manually by hand into 8 audio classes: cheering, slient, laughter, singing, music, other, talking, and broken\_link. Pruning out the broken links and duplicates, 677 unique videos with average length of 5.44 seconds (totaling $\sim 5$ GB for video and audio) were re\-classified into music and speech (approx. half in each). This data set acts as a small sample of the entire population of video this project targets. It contained numerous different spoken languages, genres of music and audio environments. Each audio file was encoded at $\SI{44.1}{\kilo\hertz}$.
      }
      \mySubSubSection{MIREX Data}{
      \label{sec:mirexdataset}
        The Music Information Retrieval Evaluation eXchange (MIREX) committee holds competitions each year on a variety of topics including music/speech classification and detection \cite{mirexhome}. The second data set considered for this report was the dataset used by that competition. It is the ``Music Speech'' dataset hosted by MARSYAS (Music Analysis, Retrival and Synthesis For Audio Signals) \cite{marsyas}. The MARSYAS data set consists of 120 audio clips each 30 seconds long with 60 belonging to each class. Each audio file was encoded at $\SI{22.05}{\kilo\hertz}$.
      }
    }
    \mySubSection{Results}{
      The results of the binary classification problem dictated in section \ref{sec:binclass} are found in this section. An interpretation of the results and their implications will follow in section \ref{sec:interp}.\\
      Note that the feature names were encoded with suffixes to denote the way they were aggregated. For example:
      \begin{itemize}
        \item MFCC\_2\_var: The variance of the 2nd MFCCs out of 13 across all windows of the clip.
        \item Mean\_mean: The average value of the spectral mean across all windows of the clip.
        \item Chroma\_0\_bpm: The beats-per-minute (BPM) predicted using the 0th Chroma Coefficient.
        \item Zero Crossing Rate\_bpm\_ratio: The BPM confidence associated with the BPM prediction while using the zero crossing rate.
      \end{itemize}
      \mySubSubSection{Feature Rankings}{
        Tables \ref{tab:sfsvmsocial} and \ref{tab:sfsvmmirex} are the single feature SVM classification accuracies after $10$-fold cross-validation on each of the $154$ features.
      }
      \mySubSubSection{Classification Models}{
        Tables \ref{tab:svmsocial} and \ref{tab:svmmirex} are the single feature SVM classification accuracies after $10$-fold cross-validation on each of the $154$ features.
      }
    }

        \begin{table}
          \definecolor{Gray}{gray}{0.85}
          \begin{center}
          \caption{Single Feature SVM with $10$-fold Cross-Validation Rankings for Social Data Set}
          \label{tab:sfsvmsocial}
          \vspace{0.05in}
          \begin{tabular}{|l|l|l|}
            \hline
            \rowcolor{Gray}
             \textbf{Rank} & \textbf{Feature Name} & \textbf{Classification Accuracy} \\
            \hline
             1 & MFCC\_0\_mean & 0.752212 \\
            \hline
             2 & Root Mean Squared\_mean & 0.716814 \\
            \hline
             3 & Mean\_mean & 0.713864  \\
            \hline
             4 & Chroma\_5\_mean & 0.699115 \\
            \hline
             5 & Variance\_mean & 0.693215 \\
            \hline
             6 & Chroma\_2\_mean & 0.663717 \\
            \hline
             7 & Energy\_bpm & 0.651917 \\
            \hline
             8 & Chroma\_5\_bpm & 0.648968 \\
            \hline
             9 & MFCC\_1\_var & 0.646018 \\
            \hline
             10 & Chroma\_9\_mean & 0.643068 \\
            \hline
             \multicolumn{3}{|c|}{$\cdots$}  \\
            \hline
             149 & MFCC\_3\_bpm & 0.513274 \\
            \hline
             150 & Kurtosis\_bpm & 0.510324 \\
            \hline
             151 & MFCC\_1\_mean & 0.507374 \\
            \hline
             152 & MFCC\_2\_bpm & 0.507377 \\
            \hline
             153 & MFCC\_6\_bpm & 0.498525 \\
            \hline
             154 & MFCC\_0\_var & 0.483776 \\
            \hline
          \end{tabular}
          \end{center}
        \end{table}

      \begin{table}
          \definecolor{Gray}{gray}{0.85}
          \begin{center}
          \caption{Single Feature SVM with $10$-fold Cross-Validation Rankings for MIREX Data Set}
          \label{tab:sfsvmmirex}
          \vspace{0.05in}
          \begin{tabular}{|l|l|l|}
            \hline
            \rowcolor{Gray}
             \textbf{Rank} & \textbf{Feature Name} & \textbf{Classification Accuracy} \\
            \hline
                 1 & MFCC\_2\_var                    &  0.9375 \\
            \hline
                 2 & MFCC\_0\_mean                   &  0.84375 \\
            \hline
                 3 & MFCC\_0\_var                    &  0.84375 \\
            \hline
                 4 & MFCC\_3\_var                    &  0.828125 \\
            \hline
                 5 & Skewness\_var                   &  0.78125 \\
            \hline
                 6 & MFCC\_3\_mean                   &  0.734375 \\
            \hline
                 7 & Spectral Centroid\_bpm\_ratio   &  0.71875 \\
            \hline
                 8 & MFCC\_1\_mean                   &  0.703125 \\
            \hline
                 9 & Mean\_mean                      &  0.6875 \\
            \hline
                 10 & MFCC\_1\_var                   &  0.6875 \\
            \hline
             \multicolumn{3}{|c|}{$\cdots$}  \\
            \hline
               148 & Chroma\_0\_bpm                  &  0.6875 \\
            \hline
               149 & Zero Crossing Rate\_mean        &  0.40625 \\
            \hline
               150 & Zero Crossing Rate\_bpm\_ratio  &  0.40625 \\
            \hline
               151 & Chroma\_8\_mean                 &  0.390625 \\
            \hline
               152 & Variance\_bpm\_ratio            &  0.390625 \\
            \hline
               153 & MFCC\_0\_bpm\_ratio             &  0.359375 \\
            \hline
               154 & MFCC\_5\_bpm                    &  0.328125 \\
            \hline
          \end{tabular}
          \end{center}
        \end{table}

        \begin{table}
          \definecolor{Gray}{gray}{0.85}
        \caption{Precision, Recall and F1-Scores for Social Data Set across 4 SVM - RBF Models}
        \label{tab:svmsocial}
        \begin{center}
        \begin{tabular}{|l|l|l|l|}
        \hline
        \rowcolor{Gray}
        \multicolumn{4}{|c|}{\textbf{SVM Model Social Data Results}} \\
        \hline
        {} & Precision & Recall & F1-score \\
        \hline
        \multicolumn{4}{|l|}{$k=1$} \\
        \multicolumn{4}{|l|}{1. MFCC\_0\_mean} \\
        \hline
        music & 0.83 & 0.50 & 0.62 \\
        speech & 0.72 & 0.93 & 0.81 \\
        avg / total & 0.78 & 0.72 & 0.72 \\
        \hline
        \multicolumn{4}{|l|}{$k=2$} \\
        \multicolumn{4}{|l|}{1. MFCC\_0\_mean} \\
        \multicolumn{4}{|l|}{2. Root Mean Squared\_mean} \\
        \hline
        music & 0.79 & 0.60 & 0.68 \\
        speech & 0.82 & 0.88 & 0.85 \\
        avg / total & 0.81 & 0.74 & 0.77 \\
        \hline
        \multicolumn{4}{|l|}{$k=4$} \\
        \multicolumn{4}{|l|}{1. MFCC\_0\_mean} \\
        \multicolumn{4}{|l|}{2. Root Mean Squared\_mean} \\
        \multicolumn{4}{|l|}{3. Mean\_mean} \\
        \multicolumn{4}{|l|}{4. Chroma\_5\_mean} \\
        \hline
        music & 0.74 & 0.63 & 0.68 \\
        speech & 0.74 & 0.83 & 0.78 \\
        avg / total & 0.74 & 0.73 & 0.73 \\
        \hline
        \multicolumn{4}{|l|}{$k=$all} \\
        \hline
        music & 1.00 & 0.01 & 0.01 \\
        speech & 0.54 & 1.00 & 0.70 \\
        avg / total & 0.77 & 0.51 & 0.36 \\
        \hline
        \end{tabular}
        \end{center}
        \end{table}

         \begin{table}
          \definecolor{Gray}{gray}{0.85}
        \caption{Precision, Recall and F1-Scores for MIREX Data Set across 4 SVM - RBF Models}
        \label{tab:svmmirex}
        \begin{center}
        \begin{tabular}{|l|l|l|l|}
        \hline
        \rowcolor{Gray}
        \multicolumn{4}{|c|}{\textbf{SVM Model MIREX Data Results}} \\
        \hline
        {} & Precision & Recall & F1-score \\
        \hline
        \multicolumn{4}{|l|}{$k=1$} \\
        \multicolumn{4}{|l|}{1. MFCC\_2\_var} \\
        \hline
        music & 0.92 & 0.97 & 0.94 \\
        speech & 0.96 & 0.90 & 0.93 \\
        avg / total & 0.94 & 0.94 & 0.94 \\
        \hline
        \multicolumn{4}{|l|}{$k=2$} \\
        \multicolumn{4}{|l|}{1. MFCC\_2\_var} \\
        \multicolumn{4}{|l|}{2. MFCC\_0\_var} \\
        \hline
        music & 0.93 & 0.97 & 0.95 \\
        speech & 0.94 & 0.94 & 0.94 \\
        avg / total & 0.94 & 0.96 & 0.95 \\
        \hline
        \multicolumn{4}{|l|}{$k=4$} \\
        \multicolumn{4}{|l|}{1. MFCC\_2\_var} \\
        \multicolumn{4}{|l|}{2. MFCC\_0\_var} \\
        \multicolumn{4}{|l|}{3. MFCC\_0\_mean} \\
        \multicolumn{4}{|l|}{4. MFCC\_3\_var} \\
        \hline
        music & 0.94 & 0.85 & 0.89 \\
        speech & 0.85 & 0.93 & 0.89 \\
        avg / total & 0.90 & 0.89 & 0.89 \\
        \hline
        \multicolumn{4}{|l|}{$k=$all} \\
        \hline
        music & 0.48 & 1.00 & 0.65 \\
        speech & 0.00 & 0.00 & 0.00 \\
        avg / total & 0.24 & 0.50 & 0.33 \\
        \hline
        \end{tabular}
        \end{center}
        \end{table}
    \mySubSection{Interpretation}{
    \label{sec:interp}
      In order to digest the results of tables \ref{tab:sfsvmsocial}, \ref{tab:sfsvmmirex}, \ref{tab:svmsocial}, and \ref{tab:svmmirex}, dicussions will be broken into four parts. First, a comparison between the social and MIREX data sets. Second, the success of MFCCs will be revealed and causes for this justified. Moreover, the results of table \ref{tab:svmmirex} will be compared to the winners of the 2015 MIREX competition. Finally, the poor performance of the beat extraction will be rationalized.
      \mySubSubSection{Social vs. MIREX Data Sets}{
      At an initial glance at tables \ref{tab:sfsvmsocial} and \ref{tab:sfsvmmirex} is evident that the features considered perform much differently. For the social data set (see table \ref{tab:sfsvmsocial}) the best features were able to perform with classification accuracy of around $\sim 60\% - 75 \%$ while for the MIREX data set (see table \ref{tab:sfsvmmirex}) the best features achieved classification accuracies of around $\sim 80\% - 93\%$. This large difference was to be expected. The social data set, described in section \ref{sec:socialdataset}, was composed of 677 videos from all across the internet. Upon listening to a sample of them by hand, one will recognize that these audio channels were not \textit{uniform}. The social data set is real-world data; it is composed of all sorts of noisy signals. Furthermore the videos were manually classified into the class (speech or music) that suited it best. A large portion of audio clips had portions of speech (not singing) and portions of music, sometimes overlapping.This analysis outlines a key limitation of performing audio analysis on social media data. Specifically, there is never a clearly defined audio environment and performing high-level audio classification will never be perfect; this classification task is too idealized. Nonetheless, a maximum F1-score of 0.77 (table \ref{tab:svmsocial}) is very promising and users of Sysomos products are expected to be understanding of the inherent difficulties with performing this type of classification. The relatively poor classification performance of the social data set is contrasted with tables \ref{tab:svmmirex} and \ref{tab:sfsvmmirex}. A maximum F1-score of 0.95 reveals how much more seperable the MIREX data set is.
      }
      \mySubSubSection{MFCCs Perform Well}{
      Secondly, table \ref{tab:sfsvmmirex} illuminates the success of MFCCs. Out of the top 10 features, 7 are derivative of the first order MFCCs. This means  MFCCs out-perform most other features at audio classification tasks. By admission however, the computation and motivation for MFCCs is much more complicated as outlined in section \ref{sec:MFCCs}. The other features mentioned in section \ref{sec:featurelist} are statistical properties of the signal and have no basis for audio processing. None of them mimic the way humans perceive and interpret sounds (maybe with exception of Chroma Coefficients). It is important to notice that the top-performing features in table \ref{tab:sfsvmmirex} are all mostly variance aggregations on first order features. Intuitively, the variance of the MFCC series should indicate the dynamic range of phonemes, and by extension, words spoken. Unlike the mean of the MFCC series, which does not characterize changes in speech over time. The result, discovered through programmatic analysis validates the reoccuring use of MFCC variances and standard deviation features by top researchers that win the annual MIREX competition \cite{mirexwinnersall, mirexwinners1, mirex2015winners}.
      }
      \mySubSubSection{Comparison to MIREX Winners}{
      \label{sec:comparisonmirex}
      The 2015 winners of the music/speech classification task on the MIREX data set was lead by a team at the Institute of Technology Kharaqpur, India \cite{mirex2015winners}. They used the standard deviation of MFCCs taken over blocks (bandwidths) of the audio clip and used Gaussian Mixture Models (GMMs) in order to achieve results results outlined in table \ref{tab:mirexwinners}. In comparison, the Indian team of researchers managed to get classification accuracies around $98.43\%$, whereas this report's methods managed to achieve accuracies of $95\%$. Given that the methods were very similar, how did the winners manage to get such high accuracy? The answer brings to light a very important part of MFCC computation: there are countless \textit{free parameters} associated with the generation of the MFCC values. To list a few: the number of triangular filters (firgure \ref{fig:triangularwindowing}), the shape of the filters, the frequency range of the filter banks, the number of coefficients to retain after the discrete cosine transform ($N\tsb{mfcc}$), the window size ($N$), the apodization window function, etc. The MFCC computation used in this report is very uneducated. A quick survey of parameters was taken and used without modification. The current state-of-the-art research of MFCCs involves determining which free parameters to change in order to optimize the classification accuracy \cite{mirexwinnersall, mirexwinners1, mirex2015winners}. In doing so, researchers learn a lot about the nature of how humans understand speech. Consequently, the classification accuracy can still be improved past the values found in table \ref{tab:svmmirex} with further fine-tuning.
      }
      \myTable{0.7}{findings/mirexwinnerstable.png}{\label{tab:mirexwinners} Classification accuracy results of 2015 MIREX winners. Taken from \cite{mirex2015winners}.}
      \mySubSubSection{Fast Beat Extraction is Terrible}{
      Careful analysis of tables \ref{tab:sfsvmsocial} and \ref{tab:sfsvmmirex}, particularily the features ranked at the bottom of the tables demonstrates how poorly the beat extraction aggregation features (see section \ref{sec:bpm}) performed. Across both data sets, BPM features are consistently failing to achieve higher than 60\% classification accuracy. These results come somewhat unsuprisingly, as the beat extraction algorithm outlined in section \ref{sec:bpm} was designed to be very fast, at the cost of being an approximation of the true BPM. One explanation of this phenomena could be that the data being considered is of a very short time scale (5.5s for social, 30s for mirex). The algorithm is expected to perform better for longer audio signals just by construction; as time progresses, a larger and larger histogram is generated. Alternatively, these results could be indicating that there is no strong correlation between speech, music and extracted beats. However, research suggests this is less likely \cite{tempovariation}. Regardless, further analysis will have to be conducted in order to fully determine if beat extraction has a place in audio environment classification or not. \\

      Interestingly, in table \ref{tab:sfsvmmirex}, the BPM confidence of the spectral centroid achieved classification accuracy of $72\%$. Research into the automatic excitement detection of baseball game video found that the standard deviation of the spectral centroid can classify audio environments as high as $80.1\%$ \cite{baseball}. Under this study, BPM confidence out-performed the standard deviation (variance) aggregation. These results can be qualitatively justified for the context of excitement detection; the tempo of audience cheering at a baseball game might be a better measure of excitement than current research suggests \cite{baseball}.
      }
    }
  }
}

% Conclusions
\newcommand{\conclusions}{
  \newpage
  \section{Conclusions}{
    \conclusion{MFCCs model human interaction with audio.}{
      MFCCs effectively mimic the human behaviour of listening to speech. They utilize the logarithmic perception of both pitch and loudness, the typical frequency ranges of human speech, and the harmonics generated when humans speak phonemes. This is quantitatively illustrated where variances in MFCC values are used to classify MIREX music/speech data sets to an accuracy of $95\%$.
    }
    \conclusion{MFCCs have the ability to perform well in non-speech modelling.}{
      Typically, MFCCs are used for acoustic modelling of automatic speech recognition tasks. In this study, MFCCs are used to build SVM models of 2 features to achieve near state-of-the-art music/speech classification accuracies, thus justifying their use in other acoustic modelling tasks.
    }
    \conclusion{MFCCs are the best features considered for speech modelling.}{
      When compared against 25 other statistical and musical features of an audio signal in both the time and frequency domains, MFCCs outperform all others with few exceptions. This indicates the MFCCs are the best features to use for acoustic modelling tasks.
    }
    \conclusion{Beat extraction is not an effective tool for audio environment classification.}{
      For both data sets considered, using beat extraction on time-based feature series to generate BPM and BPM confidence features will produce less than ideal results. Few beat extraction features managed to achieve a classification accuracy of more than $60\%$ on music/speech data sets, while some managed to perform worst than random guessing (50\%).
    }
  }
}

% Recommendations
\newcommand{\recommendations}{
  \newpage
  \section{Recommendations}{
    \recommendation{Fine-tune free parameters of MFCC computation.}{
      As outlined in \ref{sec:comparisonmirex}, MFCCs have a lot of free parameters to be chosen at implementation time. Fine-tuning and adjusting these parameters to maximize the performance of MFCCs can produce higher than demonstrated classification accuracies.
    }
    \recommendation{Build full pipline using complete model.}{
      This project demonstrated why MFCCs are both theoretically and experimentally excellent models for speech production and understanding. By using MFCCs as an acoustic model, a full pipeline should be designed to integrate with a language model and pronounciation model in order to perform end-to-end automatic speech recognition and other speech modelling tasks.
    }
    \recommendation{Perform scalability analysis on pipeline.}{
      This report did no analysis or tests of computational scalability. It is possible that although MFCCs are best in an academic setting, their computation is too expensive to be used on a large subset of all social video. Scalability tests should be performed on the proposed pipeline.
    }
    \recommendation{Explore deep learning techniques to improve feature optimizations.}{
      Deep learning techniques, specifically Convolution Neural Networks (CNNs) could potentially expose audio features that could out-perform MFCCs. Exploring existing research into feature learning on audio signals is necessary if greater than $75\%$ accuracy is desired for social audio.
    }
  }
}

% References
\newcommand{\references}{
  \makereferences{
      \bibitem{MFCCfoundation1}
      P. Mermelstein (1976), Distance measures for speech recognition, psychological and instrumental, in Pattern Recognition and Artificial Intelligence, C. H. Chen, Ed., pp. 374–388. Academic, New York.
      \bibitem{melfoundations}
      Stevens, Stanley Smith; Volkmann; John; \& Newman, Edwin B. (1937). A scale for the measurement of the psychological magnitude pitch. Journal of the Acoustical Society of America 8 (3): 185–190.
      \bibitem{MFCCfoundation2}
      S.B. Davis, and P. Mermelstein (1980), Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences, in IEEE Transactions on Acoustics, Speech, and Signal Processing, 28(4), pp. 357–366.
      \bibitem{MFCCfoundation3}
      J. S. Bridle and M. D. Brown (1974), An Experimental Automatic Word-Recognition System, JSRU Report No. 1003, Joint Speech Research Unit, Ruislip, England.
      \bibitem{hamming}
      Weisstein, Eric W. Hamming Function. From MathWorld-A Wolfram Web Resource. http://mathworld.wolfram.com/HammingFunction.html
      \bibitem{baseball}
      H. Boril, A. Sangwan, T. Hasan, J. H. L. Hansen. Automatic Excitement-Level Detection for Sports Highlights Generation. (2010) Center for Robust Speech Systems (CRSS), University of Texas.
      \bibitem{ms}
      W. Labov and M. Baranowski (8 Nov., 2004) 50 msec, submitted to Language Variation and Change. University of Pennylvannia.
      \bibitem{fastfourier}
      Weisstein, Eric W. Fast Fourier Transform. From MathWorld-A Wolfram Web Resource. http://mathworld.wolfram.com/FastFourierTransform.html
      \bibitem{marsyas}
      Data Sets \- Music Speech. (n.d.). Marsyas \- Music Analysis, Retrieval and Synthesis For Audio Signals. Retrieved Jan. 12, 2016, from http://marsyasweb.appspot.com/download/data\_sets/
      \bibitem{mirexwinnersall}
      2015:Music/Speech Classification and Detection Results. Retrieved Jan. 14, 2016, from http://www.music-ir.org/mirex/wiki/2015:Music/Speech\_Classification\_and\_Detection\_Results
      \bibitem{multimodal}
      J. Ngiam, A. Khosla, M. Kim. Multimodal Deep Learning. (n.d.). Department of Music, Standford University. Retrieved Dec., 11, 2015 from http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf
      \bibitem{mirexwinners1}
      J. Schuluter. Music/Speech Classification and Detection Mirex Submission. Austrian Research Institue for Artificial Intelligence, Vienna. Retrieved Jan. 14, 2016, from http://www.music-ir.org/mirex/abstracts/2015/JS2.pdf
      \bibitem{discretefourier}
      Weisstein, Eric W. Discrete Fourier Transform. From MathWorld-A Wolfram Web Resource. http://mathworld.wolfram.com/DiscreteFourierTransform.html
      \bibitem{tempovariation}
      J. Trouvain. Tempo Variation in Speech Production: Implication for Speech Synthesis. (April 2003).
      \bibitem{sysomos}
      Sysomos: Social Media Monitoring Tools (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://sysomos.com/
      \bibitem{dftox}
      The Discrete Fourier Transform. (n.d.). Retrieved Jan. 13, 2016, from http://www.robots.ox.ac.uk/~sjrob/Teaching/SP/l7.pdf
      \bibitem{twitter}
      Twitter (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://twitter.com/
      \bibitem{facebook}
      Facebook (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://www.facebook.com/
      \bibitem{vine}
      Vine (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://vine.co/
      \bibitem{instagram}
      Instagram (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://www.instagram.com/?hl=en
      \bibitem{tumblr}
      Tumblr (10 Jan. 2016) Retrieved 10 Jan. 2016 from https://www.tumblr.com/
      \bibitem{peakdetect}
      E. Billauer. peakdet: Peak detection using MATLAB (2012) Retrived from http://billauer.co.il/peakdet.html
      \bibitem{pacjulius}
      Open-Source Large Vocabulary CSR Engine Julius. Julius. (2014) Retrieved Dec. 13, 2015 from http://julius.osdn.jp/en\_index.php?q=index-en.html\#documentation
      \bibitem{paccmu}
      CMUSphinx Wiki (2015) Retrived Dec. 13, 2015 from http://cmusphinx.sourceforge.net/wiki/
      \bibitem{packaldi}
      Kaldi Documentation Retrived Dec. 13, 2015 from http://kaldi.sourceforge.net/
      \bibitem{pacdragon}
      Dragon Speech Recognition Software. NUANCE. Retrived Dec. 13, 2015 from http://www.nuance.com/dragon/index.htm
      \bibitem{ftest}
      F Statistic: Definition and How to find it. Statistics How To (n.d.). Retrieved Jan. 13, 2016 from http://www.statisticshowto.com/f-statistic/
      \bibitem{sound}
      Feynman, R., \& Leighton, R. (1963). Sound. The wave equation. In The Feynman lectures on physics (New Millennium ed., Vol. 3). Reading, Mass.: Addison-Wesley Pub.
      \bibitem{cap3play}
      Video Captioning + Transcription + Subtitling. 3PlayMedia. Retrieved Dec. 13, 2015 from http://www.3playmedia.com/
      \bibitem{capcaption}
      About Automatic Sync Technologies. CaptionSync. Retrived Dec. 13, 2015 from http://www.automaticsync.com/captionsync/
      \bibitem{capamara}
      Amara - Caption, translate, subtitle and transcribe video. Retrieved Dec. 13, 2015 from https://www.amara.org/en/
      \bibitem{capvoicebase}
      APIs for speech recognition and speech analytics. VoiceBase. Retrieved from Dec. 13, 2015 from https://www.voicebase.com/
      \bibitem{humanvoicerange}
      Baken, R. J. (1987). Clinical Measurement of Speech and Voice. London: Taylor and Francis Ltd. (pp. 177)
      \bibitem{comb}
      B. A. Hutchins, Jr. and W. H. Ku. An Adapting Delay Comb Filter for the Resotration of Audio Signals Badly Corrupted with a Periodic Signal of Slowing Changing Frequency. Cornell University, School of Electrical Engineering.
      \bibitem{rplot}
      L. Zhang, C. Bao, X. Liu. Audio Classification Algorithm Based on Nonlinear Chracteristics Analysis. Speech and Audio Signal Processing Laboratory, Beijing University of Technology, Beijing.
      \bibitem{ffmpeg}
      A complete, cross-platform solution to record, convert and stream audio and video. Retrieved Dec. 17, 2016 from https://www.ffmpeg.org/
      \bibitem{tempo}
      E. D. Scheirer. Tempo and Beat Analysis of Musical Signals. (n.d.). Machine Listening Group, MIT Media Laboratory.
      \bibitem{siri}
      Siri. Yout wish is its command. Apple Inc. (2016) Retrieved from http://www.apple.com/ca/ios/siri/
      \bibitem{computingmfcc}
      S. Molau, M. Pitz, R. Schluter, and H. Ney. Computing Mel-Frequency Cepstral Coefficients On The Power Spectrum. (n.d.). Computer Science Department, University of Technology, Germany
      \bibitem{sysomosgaze}
      Sysomos: Gaze, See Your Brand in a Whole New Way (2015). Retrieved Jan. 14, 2016 from https://sysomos.com/products/sysomos-gaze
      \bibitem{apigoogle}
      Web Speech API Demonstration (n.d.). Retrieved Dec. 13, 2015 from https://www.google.com/intl/en/chrome/demos/speech.html
      \bibitem{apiwit}
      wit.ai Natural Language for Developers (2015). Retrieved Dec. 13, 2015 from https://wit.ai/
      \bibitem{apiatt}
      AT\&T Speech to Text API Documentation (2015). Retrieved Dec. 13, 2015 from http://developer.att.com/apis/speech/docs
      \bibitem{apiibm}
      IBM Watson Developer Cloud Speech to Text (2015). Retrieved Dec. 13, 2015 from http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/speech-to-text.html
      \bibitem{humanhearing}
      High Frequency Range Test (8-22kHz). (n.d.). Retrieved Jan. 12, 2016 from http://www.audiocheck.net/audiotests\_frequencycheckhigh.php
      \bibitem{asr}
      S. Furui. Automatic Speech Recognition and It's Application To Information Extraction (n.d.). Tokyo Institute of Technology
      \bibitem{nyquist}
      Weisstein, Eric W. Nyquist Frequency. From MathWorld-A Wolfram Web Resource. http://mathworld.wolfram.com/NyquistFrequency.html Retrieved 10 Jan. 2016
      \bibitem{mirexhome}
      Music Information Retrieval Evaluation eXchange (MIREX) Home. Retrived Jan. 12, 2016 from http://www.music\-ir.org/mirex/wiki/MIREX\_HOME
      \bibitem{cortana}
      What is Cortana? Microsoft (2016) Retrieved from http://windows.microsoft.com/en-ca/windows-10/getstarted-what-is-cortana.
      \bibitem{speakergender}
      Z. Ma, E. Fokoue. Speaker Gender Recognition via MFCCs and SVMs. (2013) Center for Quality and Applied Statistics.
      \bibitem{temporallearning}
      S. Renals, M. Hocbberg, and T. Robinson. Learning Temporal Dependencies in Connectionist Speech Recognition. Cambridge University Engineering Department
      \bibitem{temporaldiflearning}
      R. S. Sutton, A. G. Barto: Reinforcement Learning: An Introduction. MIT Press, 1998.
      \bibitem{mirex2015winners}
      V. Ghodasara, D. S. Naser, S. Waldekar, G. Saha. Speech/Music Classification Using Block Based MFCC Features. (2015) Electronics \& Electrical Communication Engineering Department, Indian Institute of Technology Kharaqpur, India.
      \bibitem{labelName}
      Champion, R., Paci, T. \& Vardon, J. (2012). PD 2: Critical Reflection and Report Writing. Retrieved 1 March, 2012 from https://learn.uwaterloo.ca/d2l/le/content/80224/viewContent/605550/View
      \textbf{Note:} \cite{labelName} was referenced to format this report.
  }
}

\begin{document}
\createtitlepage
\letterofsubmittal
\mytableofcontents
\summary
\introduction
\analysis
\conclusions
\recommendations
\references
\end{document}